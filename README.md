Весь код находится в папке `src`.
Для запуска требуется Python>=3.8.    
Все результаты профилирования опубликованы в файлах `profiling_results.txt`, а сценарий профилирования - в `profiling.py` соответствующих пакетов. Для измерения памяти использовался `memory_profiler`, поэтому для выполнения профилирования по памяти его необходимо установить и навесить на функцию `main` декоратор `@profile`.     
Тесты писал в минимале, для тестирования основного функционала. Обычно стараюсь покрывать код полностью.  
Приложения могут запускаться как самостоятельные приложения, интерпретатор прописан как shebang.  

### Долевое строительство
*Сложность алгоритма*: O(n)
*Сложность задачи (1-10)*: 1
*Затраченное время*: 
 - чтение задачи, анализ, проработка алгоритма и структуры программы ~30м
 - реализация драфтовой рабочей версии ~20-30м
 - дальнейшее причесывание, оптимизации, две версии на базе Decimal и float, профилирование ~2ч  
Скрипт обрабатывает данные до 100 000 не более 4 секунд на конфигурации, указанной в `profiling_results.txt`.  
Я сделал 2 версии: на базе Decimal (пакет `fraction_percent_calculation_decimal`) и на базе `float`. По скорости на больших данных там все примерно одинаково, а вот по памяти использование `Decimal` начинает проигрывать примерно раза в 2 на больших объемах данных.  

### Мегатрейдер
*Сложность алгоритма*: O(n*log n)
*Сложность задачи (1-10)*: 7
*Затраченное время*: 
 - чтение задачи, анализ, проработка алгоритма и структуры программы ~1ч
 - реализация драфтовой рабочей версии ~1ч
 - дальнейшее причесывание, оптимизации, профилирование ~3часа  
В скрипте используется питоновская сортировка (timsort), на что по сути и уходит основное процессорное 
время на больших объемах данных, поэтому сложность алгоритма выражается сложностью алгоритма сортировки.  
При попытке заменить `Decimal` на `float` время выполнения практически не изменилось, а память уменьшилась на 1Mib при 3000 элементов.